[2023-04-30 06:05:23,473] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-27T00:00:00+00:00 [queued]>
[2023-04-30 06:05:23,658] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-27T00:00:00+00:00 [queued]>
[2023-04-30 06:05:23,660] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 06:05:23,664] {taskinstance.py:1357} INFO - Starting attempt 1 of 6
[2023-04-30 06:05:23,667] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 06:05:24,514] {taskinstance.py:1377} INFO - Executing <Task(S3KeySensor): sensor_s3_obj> on 2023-04-27 00:00:00+00:00
[2023-04-30 06:05:24,607] {standard_task_runner.py:52} INFO - Started process 686 to run task
[2023-04-30 06:05:24,691] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'dag_etl', 'sensor_s3_obj', 'scheduled__2023-04-27T00:00:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/dag_etl_taskflow.py', '--cfg-path', '/tmp/tmphx60hx5z', '--error-file', '/tmp/tmpdquqyhyc']
[2023-04-30 06:05:24,713] {standard_task_runner.py:80} INFO - Job 5: Subtask sensor_s3_obj
[2023-04-30 06:05:27,056] {task_command.py:370} INFO - Running <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-27T00:00:00+00:00 [running]> on host 295d97741f96
[2023-04-30 06:05:31,739] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Victor
AIRFLOW_CTX_DAG_ID=dag_etl
AIRFLOW_CTX_TASK_ID=sensor_s3_obj
AIRFLOW_CTX_EXECUTION_DATE=2023-04-27T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-27T00:00:00+00:00
[2023-04-30 06:05:31,786] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 06:05:32,169] {base.py:68} INFO - Using connection ID 'minio_conn' for task execution.
[2023-04-30 06:05:32,184] {base_aws.py:206} INFO - Credentials retrieved from login
[2023-04-30 06:05:34,214] {base.py:301} INFO - Success criteria met. Exiting.
[2023-04-30 06:05:34,550] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=dag_etl, task_id=sensor_s3_obj, execution_date=20230427T000000, start_date=20230430T060523, end_date=20230430T060534
[2023-04-30 06:05:34,988] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-04-30 06:05:37,701] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-04-30 07:02:03,024] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-27T00:00:00+00:00 [queued]>
[2023-04-30 07:02:03,106] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-27T00:00:00+00:00 [queued]>
[2023-04-30 07:02:03,114] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 07:02:03,115] {taskinstance.py:1357} INFO - Starting attempt 1 of 6
[2023-04-30 07:02:03,116] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 07:02:03,320] {taskinstance.py:1377} INFO - Executing <Task(S3KeySensor): sensor_s3_obj> on 2023-04-27 00:00:00+00:00
[2023-04-30 07:02:03,331] {standard_task_runner.py:52} INFO - Started process 768 to run task
[2023-04-30 07:02:03,354] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'dag_etl', 'sensor_s3_obj', 'scheduled__2023-04-27T00:00:00+00:00', '--job-id', '43', '--raw', '--subdir', 'DAGS_FOLDER/dag_etl_taskflow.py', '--cfg-path', '/tmp/tmpe_mt79d_', '--error-file', '/tmp/tmpampqojkv']
[2023-04-30 07:02:03,357] {standard_task_runner.py:80} INFO - Job 43: Subtask sensor_s3_obj
[2023-04-30 07:02:04,340] {task_command.py:370} INFO - Running <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-27T00:00:00+00:00 [running]> on host 295d97741f96
[2023-04-30 07:02:05,401] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Victor
AIRFLOW_CTX_DAG_ID=dag_etl
AIRFLOW_CTX_TASK_ID=sensor_s3_obj
AIRFLOW_CTX_EXECUTION_DATE=2023-04-27T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-27T00:00:00+00:00
[2023-04-30 07:02:05,407] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 07:02:05,548] {base.py:68} INFO - Using connection ID 'minio_conn' for task execution.
[2023-04-30 07:02:05,556] {base_aws.py:206} INFO - Credentials retrieved from login
[2023-04-30 07:02:11,295] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 07:02:16,344] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 07:02:21,365] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 07:02:26,440] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 07:02:26,579] {base.py:301} INFO - Success criteria met. Exiting.
[2023-04-30 07:02:27,018] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=dag_etl, task_id=sensor_s3_obj, execution_date=20230427T000000, start_date=20230430T070203, end_date=20230430T070227
[2023-04-30 07:02:27,301] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-04-30 07:02:29,279] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-04-30 11:59:01,140] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-27T00:00:00+00:00 [queued]>
[2023-04-30 11:59:01,259] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-27T00:00:00+00:00 [queued]>
[2023-04-30 11:59:01,260] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 11:59:01,261] {taskinstance.py:1357} INFO - Starting attempt 1 of 6
[2023-04-30 11:59:01,261] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 11:59:01,432] {taskinstance.py:1377} INFO - Executing <Task(S3KeySensor): sensor_s3_obj> on 2023-04-27 00:00:00+00:00
[2023-04-30 11:59:01,448] {standard_task_runner.py:52} INFO - Started process 472 to run task
[2023-04-30 11:59:01,524] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'dag_etl', 'sensor_s3_obj', 'scheduled__2023-04-27T00:00:00+00:00', '--job-id', '234', '--raw', '--subdir', 'DAGS_FOLDER/dag_etl_taskflow.py', '--cfg-path', '/tmp/tmp4fai1ihv', '--error-file', '/tmp/tmpgctchgdt']
[2023-04-30 11:59:01,527] {standard_task_runner.py:80} INFO - Job 234: Subtask sensor_s3_obj
[2023-04-30 11:59:02,316] {task_command.py:370} INFO - Running <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-27T00:00:00+00:00 [running]> on host 295d97741f96
[2023-04-30 11:59:03,659] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Victor
AIRFLOW_CTX_DAG_ID=dag_etl
AIRFLOW_CTX_TASK_ID=sensor_s3_obj
AIRFLOW_CTX_EXECUTION_DATE=2023-04-27T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-27T00:00:00+00:00
[2023-04-30 11:59:03,663] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 11:59:03,822] {base.py:68} INFO - Using connection ID 'minio_conn' for task execution.
[2023-04-30 11:59:03,829] {base_aws.py:206} INFO - Credentials retrieved from login
[2023-04-30 11:59:04,883] {base.py:301} INFO - Success criteria met. Exiting.
[2023-04-30 11:59:05,480] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=dag_etl, task_id=sensor_s3_obj, execution_date=20230427T000000, start_date=20230430T115901, end_date=20230430T115905
[2023-04-30 11:59:05,749] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-04-30 11:59:06,643] {local_task_job.py:273} INFO - 2 downstream tasks scheduled from follow-on schedule check
[2023-04-30 15:48:23,123] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-27T00:00:00+00:00 [queued]>
[2023-04-30 15:48:23,528] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-27T00:00:00+00:00 [queued]>
[2023-04-30 15:48:23,533] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 15:48:23,541] {taskinstance.py:1357} INFO - Starting attempt 1 of 6
[2023-04-30 15:48:23,548] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 15:48:24,413] {taskinstance.py:1377} INFO - Executing <Task(S3KeySensor): sensor_s3_obj> on 2023-04-27 00:00:00+00:00
[2023-04-30 15:48:24,515] {standard_task_runner.py:52} INFO - Started process 941 to run task
[2023-04-30 15:48:24,696] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'dag_etl', 'sensor_s3_obj', 'scheduled__2023-04-27T00:00:00+00:00', '--job-id', '312', '--raw', '--subdir', 'DAGS_FOLDER/dag_etl_taskflow.py', '--cfg-path', '/tmp/tmp420n5f7g', '--error-file', '/tmp/tmprwldqr2i']
[2023-04-30 15:48:24,738] {standard_task_runner.py:80} INFO - Job 312: Subtask sensor_s3_obj
[2023-04-30 15:48:26,921] {task_command.py:370} INFO - Running <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-27T00:00:00+00:00 [running]> on host 295d97741f96
[2023-04-30 15:48:32,618] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Victor
AIRFLOW_CTX_DAG_ID=dag_etl
AIRFLOW_CTX_TASK_ID=sensor_s3_obj
AIRFLOW_CTX_EXECUTION_DATE=2023-04-27T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-27T00:00:00+00:00
[2023-04-30 15:48:32,650] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 15:48:33,194] {base.py:68} INFO - Using connection ID 'minio_conn' for task execution.
[2023-04-30 15:48:33,220] {base_aws.py:206} INFO - Credentials retrieved from login
[2023-04-30 15:48:35,660] {base.py:301} INFO - Success criteria met. Exiting.
[2023-04-30 15:48:39,389] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=dag_etl, task_id=sensor_s3_obj, execution_date=20230427T000000, start_date=20230430T154823, end_date=20230430T154839
[2023-04-30 15:48:39,805] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-04-30 15:48:42,473] {local_task_job.py:273} INFO - 2 downstream tasks scheduled from follow-on schedule check
[2023-05-02 10:36:39,365] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-27T00:00:00+00:00 [queued]>
[2023-05-02 10:36:39,547] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-27T00:00:00+00:00 [queued]>
[2023-05-02 10:36:39,551] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-02 10:36:39,552] {taskinstance.py:1357} INFO - Starting attempt 1 of 6
[2023-05-02 10:36:39,553] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-02 10:36:39,850] {taskinstance.py:1377} INFO - Executing <Task(S3KeySensor): sensor_s3_obj> on 2023-04-27 00:00:00+00:00
[2023-05-02 10:36:39,864] {standard_task_runner.py:52} INFO - Started process 843 to run task
[2023-05-02 10:36:39,935] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'dag_etl', 'sensor_s3_obj', 'scheduled__2023-04-27T00:00:00+00:00', '--job-id', '412', '--raw', '--subdir', 'DAGS_FOLDER/dag_etl_taskflow.py', '--cfg-path', '/tmp/tmpr4q9cyvg', '--error-file', '/tmp/tmpcgw3yezn']
[2023-05-02 10:36:39,940] {standard_task_runner.py:80} INFO - Job 412: Subtask sensor_s3_obj
[2023-05-02 10:36:40,845] {task_command.py:370} INFO - Running <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-27T00:00:00+00:00 [running]> on host 295d97741f96
[2023-05-02 10:36:42,661] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Victor
AIRFLOW_CTX_DAG_ID=dag_etl
AIRFLOW_CTX_TASK_ID=sensor_s3_obj
AIRFLOW_CTX_EXECUTION_DATE=2023-04-27T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-27T00:00:00+00:00
[2023-05-02 10:36:42,675] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-02 10:36:42,878] {base.py:68} INFO - Using connection ID 'minio_conn' for task execution.
[2023-05-02 10:36:42,891] {base_aws.py:206} INFO - Credentials retrieved from login
[2023-05-02 10:36:49,011] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-02 10:36:54,036] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-02 10:36:59,061] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-02 10:37:04,080] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-02 10:37:09,101] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-02 10:37:09,166] {base.py:301} INFO - Success criteria met. Exiting.
[2023-05-02 10:37:11,692] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=dag_etl, task_id=sensor_s3_obj, execution_date=20230427T000000, start_date=20230502T103639, end_date=20230502T103711
[2023-05-02 10:37:12,178] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-02 10:37:14,118] {local_task_job.py:273} INFO - 2 downstream tasks scheduled from follow-on schedule check
