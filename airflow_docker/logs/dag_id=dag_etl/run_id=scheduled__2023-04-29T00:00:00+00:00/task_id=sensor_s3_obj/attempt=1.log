[2023-04-30 06:05:30,802] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-29T00:00:00+00:00 [queued]>
[2023-04-30 06:05:31,344] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-29T00:00:00+00:00 [queued]>
[2023-04-30 06:05:31,348] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 06:05:31,351] {taskinstance.py:1357} INFO - Starting attempt 1 of 6
[2023-04-30 06:05:31,357] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 06:05:32,359] {taskinstance.py:1377} INFO - Executing <Task(S3KeySensor): sensor_s3_obj> on 2023-04-29 00:00:00+00:00
[2023-04-30 06:05:32,405] {standard_task_runner.py:52} INFO - Started process 690 to run task
[2023-04-30 06:05:32,561] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'dag_etl', 'sensor_s3_obj', 'scheduled__2023-04-29T00:00:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/dag_etl_taskflow.py', '--cfg-path', '/tmp/tmp_d0w74f_', '--error-file', '/tmp/tmpvwkx38nr']
[2023-04-30 06:05:32,580] {standard_task_runner.py:80} INFO - Job 7: Subtask sensor_s3_obj
[2023-04-30 06:05:34,196] {task_command.py:370} INFO - Running <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-29T00:00:00+00:00 [running]> on host 295d97741f96
[2023-04-30 06:05:36,165] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Victor
AIRFLOW_CTX_DAG_ID=dag_etl
AIRFLOW_CTX_TASK_ID=sensor_s3_obj
AIRFLOW_CTX_EXECUTION_DATE=2023-04-29T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-29T00:00:00+00:00
[2023-04-30 06:05:36,180] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 06:05:36,459] {base.py:68} INFO - Using connection ID 'minio_conn' for task execution.
[2023-04-30 06:05:36,494] {base_aws.py:206} INFO - Credentials retrieved from login
[2023-04-30 06:05:41,993] {base.py:301} INFO - Success criteria met. Exiting.
[2023-04-30 06:05:45,925] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=dag_etl, task_id=sensor_s3_obj, execution_date=20230429T000000, start_date=20230430T060530, end_date=20230430T060545
[2023-04-30 06:05:46,441] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-04-30 06:05:48,876] {local_task_job.py:273} INFO - 2 downstream tasks scheduled from follow-on schedule check
[2023-04-30 07:02:04,994] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-29T00:00:00+00:00 [queued]>
[2023-04-30 07:02:05,079] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-29T00:00:00+00:00 [queued]>
[2023-04-30 07:02:05,081] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 07:02:05,082] {taskinstance.py:1357} INFO - Starting attempt 1 of 6
[2023-04-30 07:02:05,083] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 07:02:05,231] {taskinstance.py:1377} INFO - Executing <Task(S3KeySensor): sensor_s3_obj> on 2023-04-29 00:00:00+00:00
[2023-04-30 07:02:05,241] {standard_task_runner.py:52} INFO - Started process 774 to run task
[2023-04-30 07:02:05,255] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'dag_etl', 'sensor_s3_obj', 'scheduled__2023-04-29T00:00:00+00:00', '--job-id', '46', '--raw', '--subdir', 'DAGS_FOLDER/dag_etl_taskflow.py', '--cfg-path', '/tmp/tmph44hop_6', '--error-file', '/tmp/tmpos4mx7dm']
[2023-04-30 07:02:05,267] {standard_task_runner.py:80} INFO - Job 46: Subtask sensor_s3_obj
[2023-04-30 07:02:05,912] {task_command.py:370} INFO - Running <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-29T00:00:00+00:00 [running]> on host 295d97741f96
[2023-04-30 07:02:07,238] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Victor
AIRFLOW_CTX_DAG_ID=dag_etl
AIRFLOW_CTX_TASK_ID=sensor_s3_obj
AIRFLOW_CTX_EXECUTION_DATE=2023-04-29T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-29T00:00:00+00:00
[2023-04-30 07:02:07,244] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 07:02:07,349] {base.py:68} INFO - Using connection ID 'minio_conn' for task execution.
[2023-04-30 07:02:07,359] {base_aws.py:206} INFO - Credentials retrieved from login
[2023-04-30 07:02:12,754] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 07:02:17,840] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 07:02:22,859] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 07:02:22,903] {base.py:301} INFO - Success criteria met. Exiting.
[2023-04-30 07:02:23,452] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=dag_etl, task_id=sensor_s3_obj, execution_date=20230429T000000, start_date=20230430T070204, end_date=20230430T070223
[2023-04-30 07:02:23,630] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-04-30 07:02:24,221] {local_task_job.py:273} INFO - 2 downstream tasks scheduled from follow-on schedule check
[2023-04-30 11:59:03,483] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-29T00:00:00+00:00 [queued]>
[2023-04-30 11:59:03,602] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-29T00:00:00+00:00 [queued]>
[2023-04-30 11:59:03,604] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 11:59:03,605] {taskinstance.py:1357} INFO - Starting attempt 1 of 6
[2023-04-30 11:59:03,607] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 11:59:03,799] {taskinstance.py:1377} INFO - Executing <Task(S3KeySensor): sensor_s3_obj> on 2023-04-29 00:00:00+00:00
[2023-04-30 11:59:03,813] {standard_task_runner.py:52} INFO - Started process 478 to run task
[2023-04-30 11:59:03,854] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'dag_etl', 'sensor_s3_obj', 'scheduled__2023-04-29T00:00:00+00:00', '--job-id', '237', '--raw', '--subdir', 'DAGS_FOLDER/dag_etl_taskflow.py', '--cfg-path', '/tmp/tmpwt7q2spf', '--error-file', '/tmp/tmpd_03hsr3']
[2023-04-30 11:59:03,881] {standard_task_runner.py:80} INFO - Job 237: Subtask sensor_s3_obj
[2023-04-30 11:59:04,802] {task_command.py:370} INFO - Running <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-29T00:00:00+00:00 [running]> on host 295d97741f96
[2023-04-30 11:59:06,258] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Victor
AIRFLOW_CTX_DAG_ID=dag_etl
AIRFLOW_CTX_TASK_ID=sensor_s3_obj
AIRFLOW_CTX_EXECUTION_DATE=2023-04-29T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-29T00:00:00+00:00
[2023-04-30 11:59:06,264] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 11:59:06,376] {base.py:68} INFO - Using connection ID 'minio_conn' for task execution.
[2023-04-30 11:59:06,385] {base_aws.py:206} INFO - Credentials retrieved from login
[2023-04-30 11:59:07,438] {base.py:301} INFO - Success criteria met. Exiting.
[2023-04-30 11:59:08,936] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=dag_etl, task_id=sensor_s3_obj, execution_date=20230429T000000, start_date=20230430T115903, end_date=20230430T115908
[2023-04-30 11:59:09,321] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-04-30 11:59:10,454] {local_task_job.py:273} INFO - 2 downstream tasks scheduled from follow-on schedule check
[2023-04-30 15:48:35,495] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-29T00:00:00+00:00 [queued]>
[2023-04-30 15:48:35,804] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-29T00:00:00+00:00 [queued]>
[2023-04-30 15:48:35,807] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 15:48:35,808] {taskinstance.py:1357} INFO - Starting attempt 1 of 6
[2023-04-30 15:48:35,811] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 15:48:36,672] {taskinstance.py:1377} INFO - Executing <Task(S3KeySensor): sensor_s3_obj> on 2023-04-29 00:00:00+00:00
[2023-04-30 15:48:36,737] {standard_task_runner.py:52} INFO - Started process 954 to run task
[2023-04-30 15:48:36,883] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'dag_etl', 'sensor_s3_obj', 'scheduled__2023-04-29T00:00:00+00:00', '--job-id', '314', '--raw', '--subdir', 'DAGS_FOLDER/dag_etl_taskflow.py', '--cfg-path', '/tmp/tmpij4bx69h', '--error-file', '/tmp/tmp2aowiolt']
[2023-04-30 15:48:36,894] {standard_task_runner.py:80} INFO - Job 314: Subtask sensor_s3_obj
[2023-04-30 15:48:39,208] {task_command.py:370} INFO - Running <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-29T00:00:00+00:00 [running]> on host 295d97741f96
[2023-04-30 15:48:42,729] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Victor
AIRFLOW_CTX_DAG_ID=dag_etl
AIRFLOW_CTX_TASK_ID=sensor_s3_obj
AIRFLOW_CTX_EXECUTION_DATE=2023-04-29T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-29T00:00:00+00:00
[2023-04-30 15:48:42,739] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 15:48:43,281] {base.py:68} INFO - Using connection ID 'minio_conn' for task execution.
[2023-04-30 15:48:43,309] {base_aws.py:206} INFO - Credentials retrieved from login
[2023-04-30 15:48:46,689] {base.py:301} INFO - Success criteria met. Exiting.
[2023-04-30 15:48:52,116] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=dag_etl, task_id=sensor_s3_obj, execution_date=20230429T000000, start_date=20230430T154835, end_date=20230430T154852
[2023-04-30 15:48:52,781] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-04-30 15:48:54,822] {local_task_job.py:273} INFO - 2 downstream tasks scheduled from follow-on schedule check
[2023-05-02 10:36:39,685] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-29T00:00:00+00:00 [queued]>
[2023-05-02 10:36:39,813] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-29T00:00:00+00:00 [queued]>
[2023-05-02 10:36:39,818] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-02 10:36:39,818] {taskinstance.py:1357} INFO - Starting attempt 1 of 6
[2023-05-02 10:36:39,819] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-02 10:36:40,174] {taskinstance.py:1377} INFO - Executing <Task(S3KeySensor): sensor_s3_obj> on 2023-04-29 00:00:00+00:00
[2023-05-02 10:36:40,203] {standard_task_runner.py:52} INFO - Started process 845 to run task
[2023-05-02 10:36:40,256] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'dag_etl', 'sensor_s3_obj', 'scheduled__2023-04-29T00:00:00+00:00', '--job-id', '413', '--raw', '--subdir', 'DAGS_FOLDER/dag_etl_taskflow.py', '--cfg-path', '/tmp/tmp36huuwd0', '--error-file', '/tmp/tmpnlovdh1v']
[2023-05-02 10:36:40,269] {standard_task_runner.py:80} INFO - Job 413: Subtask sensor_s3_obj
[2023-05-02 10:36:41,142] {task_command.py:370} INFO - Running <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-29T00:00:00+00:00 [running]> on host 295d97741f96
[2023-05-02 10:36:42,769] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Victor
AIRFLOW_CTX_DAG_ID=dag_etl
AIRFLOW_CTX_TASK_ID=sensor_s3_obj
AIRFLOW_CTX_EXECUTION_DATE=2023-04-29T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-29T00:00:00+00:00
[2023-05-02 10:36:42,776] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-02 10:36:43,048] {base.py:68} INFO - Using connection ID 'minio_conn' for task execution.
[2023-05-02 10:36:43,065] {base_aws.py:206} INFO - Credentials retrieved from login
[2023-05-02 10:36:49,144] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-02 10:36:54,197] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-02 10:36:59,218] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-02 10:37:04,235] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-02 10:37:09,262] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-02 10:37:09,302] {base.py:301} INFO - Success criteria met. Exiting.
[2023-05-02 10:37:11,696] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=dag_etl, task_id=sensor_s3_obj, execution_date=20230429T000000, start_date=20230502T103639, end_date=20230502T103711
[2023-05-02 10:37:12,210] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-02 10:37:14,132] {local_task_job.py:273} INFO - 2 downstream tasks scheduled from follow-on schedule check
[2023-05-04 05:10:23,278] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-29T00:00:00+00:00 [queued]>
[2023-05-04 05:10:23,549] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-29T00:00:00+00:00 [queued]>
[2023-05-04 05:10:23,555] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-04 05:10:23,560] {taskinstance.py:1357} INFO - Starting attempt 1 of 6
[2023-05-04 05:10:23,564] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-04 05:10:23,992] {taskinstance.py:1377} INFO - Executing <Task(S3KeySensor): sensor_s3_obj> on 2023-04-29 00:00:00+00:00
[2023-05-04 05:10:24,009] {standard_task_runner.py:52} INFO - Started process 638 to run task
[2023-05-04 05:10:24,108] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'dag_etl', 'sensor_s3_obj', 'scheduled__2023-04-29T00:00:00+00:00', '--job-id', '918', '--raw', '--subdir', 'DAGS_FOLDER/dag_etl_taskflow.py', '--cfg-path', '/tmp/tmpus56l582', '--error-file', '/tmp/tmpme7v9ein']
[2023-05-04 05:10:24,122] {standard_task_runner.py:80} INFO - Job 918: Subtask sensor_s3_obj
[2023-05-04 05:10:25,372] {task_command.py:370} INFO - Running <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-29T00:00:00+00:00 [running]> on host 295d97741f96
[2023-05-04 05:10:28,170] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Victor
AIRFLOW_CTX_DAG_ID=dag_etl
AIRFLOW_CTX_TASK_ID=sensor_s3_obj
AIRFLOW_CTX_EXECUTION_DATE=2023-04-29T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-29T00:00:00+00:00
[2023-05-04 05:10:28,180] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-04 05:10:28,662] {base.py:68} INFO - Using connection ID 'minio_conn' for task execution.
[2023-05-04 05:10:28,678] {base_aws.py:206} INFO - Credentials retrieved from login
[2023-05-04 05:10:37,606] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-04 05:10:42,822] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-04 05:10:47,859] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-04 05:10:48,015] {base.py:301} INFO - Success criteria met. Exiting.
[2023-05-04 05:10:49,392] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=dag_etl, task_id=sensor_s3_obj, execution_date=20230429T000000, start_date=20230504T051023, end_date=20230504T051049
[2023-05-04 05:10:50,581] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-04 05:10:52,503] {local_task_job.py:273} INFO - 2 downstream tasks scheduled from follow-on schedule check
