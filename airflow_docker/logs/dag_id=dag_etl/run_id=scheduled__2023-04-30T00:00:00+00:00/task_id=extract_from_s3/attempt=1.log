[2023-05-02 10:32:33,510] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.extract_from_s3 scheduled__2023-04-30T00:00:00+00:00 [queued]>
[2023-05-02 10:32:34,014] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.extract_from_s3 scheduled__2023-04-30T00:00:00+00:00 [queued]>
[2023-05-02 10:32:34,017] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-02 10:32:34,026] {taskinstance.py:1357} INFO - Starting attempt 1 of 6
[2023-05-02 10:32:34,028] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-02 10:32:34,844] {taskinstance.py:1377} INFO - Executing <Task(_PythonDecoratedOperator): extract_from_s3> on 2023-04-30 00:00:00+00:00
[2023-05-02 10:32:34,883] {standard_task_runner.py:52} INFO - Started process 447 to run task
[2023-05-02 10:32:34,956] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'dag_etl', 'extract_from_s3', 'scheduled__2023-04-30T00:00:00+00:00', '--job-id', '390', '--raw', '--subdir', 'DAGS_FOLDER/dag_etl_taskflow.py', '--cfg-path', '/tmp/tmpzqgou3yl', '--error-file', '/tmp/tmp9qmg2qik']
[2023-05-02 10:32:34,968] {standard_task_runner.py:80} INFO - Job 390: Subtask extract_from_s3
[2023-05-02 10:32:36,838] {task_command.py:370} INFO - Running <TaskInstance: dag_etl.extract_from_s3 scheduled__2023-04-30T00:00:00+00:00 [running]> on host 295d97741f96
[2023-05-02 10:32:45,252] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Victor
AIRFLOW_CTX_DAG_ID=dag_etl
AIRFLOW_CTX_TASK_ID=extract_from_s3
AIRFLOW_CTX_EXECUTION_DATE=2023-04-30T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-30T00:00:00+00:00
[2023-05-02 10:32:45,268] {s3.py:861} INFO - Downloading source S3 file from Bucket raw with path supermarket_sales.csv
[2023-05-02 10:32:46,168] {base.py:68} INFO - Using connection ID 'minio_conn' for task execution.
[2023-05-02 10:32:46,194] {base_aws.py:206} INFO - Credentials retrieved from login
[2023-05-02 10:32:56,467] {python.py:173} INFO - Done. Returned value was: /opt/***/data/***_tmp_gs1kajjr
[2023-05-02 10:32:57,271] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=dag_etl, task_id=extract_from_s3, execution_date=20230430T000000, start_date=20230502T103233, end_date=20230502T103257
[2023-05-02 10:32:58,127] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-02 10:33:03,653] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-05-02 10:37:29,019] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.extract_from_s3 scheduled__2023-04-30T00:00:00+00:00 [queued]>
[2023-05-02 10:37:29,721] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.extract_from_s3 scheduled__2023-04-30T00:00:00+00:00 [queued]>
[2023-05-02 10:37:29,744] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-02 10:37:29,748] {taskinstance.py:1357} INFO - Starting attempt 1 of 6
[2023-05-02 10:37:29,754] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-02 10:37:30,461] {taskinstance.py:1377} INFO - Executing <Task(_PythonDecoratedOperator): extract_from_s3> on 2023-04-30 00:00:00+00:00
[2023-05-02 10:37:30,519] {standard_task_runner.py:52} INFO - Started process 949 to run task
[2023-05-02 10:37:30,688] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'dag_etl', 'extract_from_s3', 'scheduled__2023-04-30T00:00:00+00:00', '--job-id', '423', '--raw', '--subdir', 'DAGS_FOLDER/dag_etl_taskflow.py', '--cfg-path', '/tmp/tmp3vfxqnno', '--error-file', '/tmp/tmpbgelyr4_']
[2023-05-02 10:37:30,771] {standard_task_runner.py:80} INFO - Job 423: Subtask extract_from_s3
[2023-05-02 10:37:33,013] {task_command.py:370} INFO - Running <TaskInstance: dag_etl.extract_from_s3 scheduled__2023-04-30T00:00:00+00:00 [running]> on host 295d97741f96
[2023-05-02 10:37:38,282] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Victor
AIRFLOW_CTX_DAG_ID=dag_etl
AIRFLOW_CTX_TASK_ID=extract_from_s3
AIRFLOW_CTX_EXECUTION_DATE=2023-04-30T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-30T00:00:00+00:00
[2023-05-02 10:37:38,296] {s3.py:861} INFO - Downloading source S3 file from Bucket raw with path supermarket_sales.csv
[2023-05-02 10:37:38,763] {base.py:68} INFO - Using connection ID 'minio_conn' for task execution.
[2023-05-02 10:37:38,776] {base_aws.py:206} INFO - Credentials retrieved from login
[2023-05-02 10:37:45,987] {python.py:173} INFO - Done. Returned value was: /opt/***/data/***_tmp_cm1rh_hm
[2023-05-02 10:37:48,612] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=dag_etl, task_id=extract_from_s3, execution_date=20230430T000000, start_date=20230502T103729, end_date=20230502T103748
[2023-05-02 10:37:49,559] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-02 10:37:52,800] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-05-04 05:11:33,687] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.extract_from_s3 scheduled__2023-04-30T00:00:00+00:00 [queued]>
[2023-05-04 05:11:34,659] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.extract_from_s3 scheduled__2023-04-30T00:00:00+00:00 [queued]>
[2023-05-04 05:11:34,682] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-04 05:11:34,689] {taskinstance.py:1357} INFO - Starting attempt 1 of 6
[2023-05-04 05:11:34,707] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-04 05:11:36,060] {taskinstance.py:1377} INFO - Executing <Task(_PythonDecoratedOperator): extract_from_s3> on 2023-04-30 00:00:00+00:00
[2023-05-04 05:11:36,207] {standard_task_runner.py:52} INFO - Started process 795 to run task
[2023-05-04 05:11:36,274] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'dag_etl', 'extract_from_s3', 'scheduled__2023-04-30T00:00:00+00:00', '--job-id', '937', '--raw', '--subdir', 'DAGS_FOLDER/dag_etl_taskflow.py', '--cfg-path', '/tmp/tmpenr3fg_q', '--error-file', '/tmp/tmpuud32dxk']
[2023-05-04 05:11:36,282] {standard_task_runner.py:80} INFO - Job 937: Subtask extract_from_s3
[2023-05-04 05:11:40,141] {task_command.py:370} INFO - Running <TaskInstance: dag_etl.extract_from_s3 scheduled__2023-04-30T00:00:00+00:00 [running]> on host 295d97741f96
[2023-05-04 05:11:48,870] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Victor
AIRFLOW_CTX_DAG_ID=dag_etl
AIRFLOW_CTX_TASK_ID=extract_from_s3
AIRFLOW_CTX_EXECUTION_DATE=2023-04-30T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-30T00:00:00+00:00
[2023-05-04 05:11:49,031] {s3.py:861} INFO - Downloading source S3 file from Bucket raw with path supermarket_sales.csv
[2023-05-04 05:11:50,131] {base.py:68} INFO - Using connection ID 'minio_conn' for task execution.
[2023-05-04 05:11:50,160] {base_aws.py:206} INFO - Credentials retrieved from login
[2023-05-04 05:11:59,935] {python.py:173} INFO - Done. Returned value was: /opt/***/data/***_tmp_j4uhhu7u
[2023-05-04 05:12:01,325] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=dag_etl, task_id=extract_from_s3, execution_date=20230430T000000, start_date=20230504T051133, end_date=20230504T051201
[2023-05-04 05:12:01,948] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-04 05:12:03,844] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
