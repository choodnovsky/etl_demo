[2023-04-30 06:05:26,181] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-28T00:00:00+00:00 [queued]>
[2023-04-30 06:05:26,679] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-28T00:00:00+00:00 [queued]>
[2023-04-30 06:05:26,683] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 06:05:26,686] {taskinstance.py:1357} INFO - Starting attempt 1 of 6
[2023-04-30 06:05:26,688] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 06:05:27,324] {taskinstance.py:1377} INFO - Executing <Task(S3KeySensor): sensor_s3_obj> on 2023-04-28 00:00:00+00:00
[2023-04-30 06:05:27,366] {standard_task_runner.py:52} INFO - Started process 688 to run task
[2023-04-30 06:05:27,444] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'dag_etl', 'sensor_s3_obj', 'scheduled__2023-04-28T00:00:00+00:00', '--job-id', '6', '--raw', '--subdir', 'DAGS_FOLDER/dag_etl_taskflow.py', '--cfg-path', '/tmp/tmpy6pfm9hy', '--error-file', '/tmp/tmpgx0212ou']
[2023-04-30 06:05:27,471] {standard_task_runner.py:80} INFO - Job 6: Subtask sensor_s3_obj
[2023-04-30 06:05:29,220] {task_command.py:370} INFO - Running <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-28T00:00:00+00:00 [running]> on host 295d97741f96
[2023-04-30 06:05:33,494] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Victor
AIRFLOW_CTX_DAG_ID=dag_etl
AIRFLOW_CTX_TASK_ID=sensor_s3_obj
AIRFLOW_CTX_EXECUTION_DATE=2023-04-28T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-28T00:00:00+00:00
[2023-04-30 06:05:33,504] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 06:05:33,824] {base.py:68} INFO - Using connection ID 'minio_conn' for task execution.
[2023-04-30 06:05:33,845] {base_aws.py:206} INFO - Credentials retrieved from login
[2023-04-30 06:05:35,424] {base.py:301} INFO - Success criteria met. Exiting.
[2023-04-30 06:05:36,265] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=dag_etl, task_id=sensor_s3_obj, execution_date=20230428T000000, start_date=20230430T060526, end_date=20230430T060536
[2023-04-30 06:05:36,635] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-04-30 06:05:40,430] {local_task_job.py:273} INFO - 2 downstream tasks scheduled from follow-on schedule check
[2023-04-30 07:02:04,703] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-28T00:00:00+00:00 [queued]>
[2023-04-30 07:02:04,818] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-28T00:00:00+00:00 [queued]>
[2023-04-30 07:02:04,822] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 07:02:04,825] {taskinstance.py:1357} INFO - Starting attempt 1 of 6
[2023-04-30 07:02:04,828] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 07:02:05,021] {taskinstance.py:1377} INFO - Executing <Task(S3KeySensor): sensor_s3_obj> on 2023-04-28 00:00:00+00:00
[2023-04-30 07:02:05,037] {standard_task_runner.py:52} INFO - Started process 772 to run task
[2023-04-30 07:02:05,077] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'dag_etl', 'sensor_s3_obj', 'scheduled__2023-04-28T00:00:00+00:00', '--job-id', '45', '--raw', '--subdir', 'DAGS_FOLDER/dag_etl_taskflow.py', '--cfg-path', '/tmp/tmpmzl88ttb', '--error-file', '/tmp/tmpf6x_w43t']
[2023-04-30 07:02:05,082] {standard_task_runner.py:80} INFO - Job 45: Subtask sensor_s3_obj
[2023-04-30 07:02:05,741] {task_command.py:370} INFO - Running <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-28T00:00:00+00:00 [running]> on host 295d97741f96
[2023-04-30 07:02:06,964] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Victor
AIRFLOW_CTX_DAG_ID=dag_etl
AIRFLOW_CTX_TASK_ID=sensor_s3_obj
AIRFLOW_CTX_EXECUTION_DATE=2023-04-28T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-28T00:00:00+00:00
[2023-04-30 07:02:06,970] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 07:02:07,102] {base.py:68} INFO - Using connection ID 'minio_conn' for task execution.
[2023-04-30 07:02:07,110] {base_aws.py:206} INFO - Credentials retrieved from login
[2023-04-30 07:02:12,570] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 07:02:17,619] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 07:02:22,638] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 07:02:22,666] {base.py:301} INFO - Success criteria met. Exiting.
[2023-04-30 07:02:22,843] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=dag_etl, task_id=sensor_s3_obj, execution_date=20230428T000000, start_date=20230430T070204, end_date=20230430T070222
[2023-04-30 07:02:23,027] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-04-30 07:02:23,570] {local_task_job.py:273} INFO - 2 downstream tasks scheduled from follow-on schedule check
[2023-04-30 11:59:01,685] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-28T00:00:00+00:00 [queued]>
[2023-04-30 11:59:01,871] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-28T00:00:00+00:00 [queued]>
[2023-04-30 11:59:01,878] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 11:59:01,879] {taskinstance.py:1357} INFO - Starting attempt 1 of 6
[2023-04-30 11:59:01,885] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 11:59:02,083] {taskinstance.py:1377} INFO - Executing <Task(S3KeySensor): sensor_s3_obj> on 2023-04-28 00:00:00+00:00
[2023-04-30 11:59:02,105] {standard_task_runner.py:52} INFO - Started process 476 to run task
[2023-04-30 11:59:02,124] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'dag_etl', 'sensor_s3_obj', 'scheduled__2023-04-28T00:00:00+00:00', '--job-id', '236', '--raw', '--subdir', 'DAGS_FOLDER/dag_etl_taskflow.py', '--cfg-path', '/tmp/tmp2m0comng', '--error-file', '/tmp/tmpnr3av_tn']
[2023-04-30 11:59:02,127] {standard_task_runner.py:80} INFO - Job 236: Subtask sensor_s3_obj
[2023-04-30 11:59:02,858] {task_command.py:370} INFO - Running <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-28T00:00:00+00:00 [running]> on host 295d97741f96
[2023-04-30 11:59:04,215] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Victor
AIRFLOW_CTX_DAG_ID=dag_etl
AIRFLOW_CTX_TASK_ID=sensor_s3_obj
AIRFLOW_CTX_EXECUTION_DATE=2023-04-28T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-28T00:00:00+00:00
[2023-04-30 11:59:04,219] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 11:59:04,388] {base.py:68} INFO - Using connection ID 'minio_conn' for task execution.
[2023-04-30 11:59:04,398] {base_aws.py:206} INFO - Credentials retrieved from login
[2023-04-30 11:59:05,317] {base.py:301} INFO - Success criteria met. Exiting.
[2023-04-30 11:59:06,017] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=dag_etl, task_id=sensor_s3_obj, execution_date=20230428T000000, start_date=20230430T115901, end_date=20230430T115906
[2023-04-30 11:59:06,295] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-04-30 11:59:07,491] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-04-30 15:48:29,297] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-28T00:00:00+00:00 [queued]>
[2023-04-30 15:48:29,478] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-28T00:00:00+00:00 [queued]>
[2023-04-30 15:48:29,481] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 15:48:29,483] {taskinstance.py:1357} INFO - Starting attempt 1 of 6
[2023-04-30 15:48:29,486] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-04-30 15:48:30,395] {taskinstance.py:1377} INFO - Executing <Task(S3KeySensor): sensor_s3_obj> on 2023-04-28 00:00:00+00:00
[2023-04-30 15:48:30,447] {standard_task_runner.py:52} INFO - Started process 946 to run task
[2023-04-30 15:48:30,500] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'dag_etl', 'sensor_s3_obj', 'scheduled__2023-04-28T00:00:00+00:00', '--job-id', '313', '--raw', '--subdir', 'DAGS_FOLDER/dag_etl_taskflow.py', '--cfg-path', '/tmp/tmpsbw0fwxa', '--error-file', '/tmp/tmpkxzo73bo']
[2023-04-30 15:48:30,510] {standard_task_runner.py:80} INFO - Job 313: Subtask sensor_s3_obj
[2023-04-30 15:48:33,772] {task_command.py:370} INFO - Running <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-28T00:00:00+00:00 [running]> on host 295d97741f96
[2023-04-30 15:48:36,586] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Victor
AIRFLOW_CTX_DAG_ID=dag_etl
AIRFLOW_CTX_TASK_ID=sensor_s3_obj
AIRFLOW_CTX_EXECUTION_DATE=2023-04-28T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-28T00:00:00+00:00
[2023-04-30 15:48:36,615] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-04-30 15:48:37,036] {base.py:68} INFO - Using connection ID 'minio_conn' for task execution.
[2023-04-30 15:48:37,058] {base_aws.py:206} INFO - Credentials retrieved from login
[2023-04-30 15:48:39,691] {base.py:301} INFO - Success criteria met. Exiting.
[2023-04-30 15:48:42,416] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=dag_etl, task_id=sensor_s3_obj, execution_date=20230428T000000, start_date=20230430T154829, end_date=20230430T154842
[2023-04-30 15:48:43,557] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-04-30 15:48:49,142] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-02 10:36:42,110] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-28T00:00:00+00:00 [queued]>
[2023-05-02 10:36:42,259] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-28T00:00:00+00:00 [queued]>
[2023-05-02 10:36:42,260] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-02 10:36:42,261] {taskinstance.py:1357} INFO - Starting attempt 1 of 6
[2023-05-02 10:36:42,262] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-02 10:36:42,543] {taskinstance.py:1377} INFO - Executing <Task(S3KeySensor): sensor_s3_obj> on 2023-04-28 00:00:00+00:00
[2023-05-02 10:36:42,566] {standard_task_runner.py:52} INFO - Started process 853 to run task
[2023-05-02 10:36:42,594] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'dag_etl', 'sensor_s3_obj', 'scheduled__2023-04-28T00:00:00+00:00', '--job-id', '414', '--raw', '--subdir', 'DAGS_FOLDER/dag_etl_taskflow.py', '--cfg-path', '/tmp/tmpu3f0l6ze', '--error-file', '/tmp/tmpu2v0uvj0']
[2023-05-02 10:36:42,605] {standard_task_runner.py:80} INFO - Job 414: Subtask sensor_s3_obj
[2023-05-02 10:36:43,683] {task_command.py:370} INFO - Running <TaskInstance: dag_etl.sensor_s3_obj scheduled__2023-04-28T00:00:00+00:00 [running]> on host 295d97741f96
[2023-05-02 10:36:45,419] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Victor
AIRFLOW_CTX_DAG_ID=dag_etl
AIRFLOW_CTX_TASK_ID=sensor_s3_obj
AIRFLOW_CTX_EXECUTION_DATE=2023-04-28T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-28T00:00:00+00:00
[2023-05-02 10:36:45,427] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-02 10:36:45,781] {base.py:68} INFO - Using connection ID 'minio_conn' for task execution.
[2023-05-02 10:36:45,789] {base_aws.py:206} INFO - Credentials retrieved from login
[2023-05-02 10:36:51,571] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-02 10:36:56,635] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-02 10:37:01,659] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-02 10:37:06,678] {s3.py:104} INFO - Poking for key : s3://raw/supermarket_sales.csv
[2023-05-02 10:37:06,699] {base.py:301} INFO - Success criteria met. Exiting.
[2023-05-02 10:37:07,029] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=dag_etl, task_id=sensor_s3_obj, execution_date=20230428T000000, start_date=20230502T103642, end_date=20230502T103707
[2023-05-02 10:37:07,154] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-02 10:37:07,605] {local_task_job.py:273} INFO - 2 downstream tasks scheduled from follow-on schedule check
